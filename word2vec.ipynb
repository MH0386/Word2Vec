{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a8361c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:27.183443Z",
     "iopub.status.busy": "2023-12-01T13:37:27.182669Z",
     "iopub.status.idle": "2023-12-01T13:37:43.394924Z",
     "shell.execute_reply": "2023-12-01T13:37:43.393706Z"
    },
    "papermill": {
     "duration": 16.227106,
     "end_time": "2023-12-01T13:37:43.398270",
     "exception": false,
     "start_time": "2023-12-01T13:37:27.171164",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.548320700Z",
     "start_time": "2023-12-01T21:03:27.602889900Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7df27f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.416191Z",
     "iopub.status.busy": "2023-12-01T13:37:43.414820Z",
     "iopub.status.idle": "2023-12-01T13:37:43.432822Z",
     "shell.execute_reply": "2023-12-01T13:37:43.431735Z"
    },
    "papermill": {
     "duration": 0.029585,
     "end_time": "2023-12-01T13:37:43.435548",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.405963",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.579239100Z",
     "start_time": "2023-12-01T21:03:35.550322Z"
    }
   },
   "outputs": [],
   "source": [
    "texts1 = \"Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors. Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word2vec can utilize either of two model architectures to produce these distributed representations of words: continuously sliding bag-of-words (CBOW) or continuously sliding skip-gram. In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus. The CBOW can be viewed as a ‘fill in the blank’ task, where the word embedding represents the way the word influences the relative probabilities of other words in the context window. Words which are semantically similar should influence these probabilities in similar ways, because semantically similar words should be used in similar contexts. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.[1][2] The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note,[3] CBOW is faster while skip-gram does a better job for infrequent words. After the model has trained, the learned word embeddings are positioned in the vector space such that words that share common contexts in the corpus — that is, words that are semantically and syntactically similar — are located close to one another in the space.[1] More dissimilar words are located farther from one another in the space.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "texts2 = 'GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.[1] Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. It is developed as an open-source project at Stanford[2] and was launched in 2014. As log-bilinear regression model for unsupervised learning of word representations, it combines the features of two model families, namely the global matrix factorization and local context window methods.[3] GloVe can be used to find relations between words like synonyms, company-product relations, zip codes and cities, etc. However, the unsupervised learning algorithm is not effective in identifying homographs, that is, words with the same spelling and different meanings. This is as the unsupervised learning algorithm calculates a single set of vectors for words with the same morphological structure.[4] The algorithm is also used by the SpaCy library to build semantic word embedding features, while computing the top list words that match with distance measures such as cosine similarity and Euclidean distance approach.[5] GloVe was also used as the word representation framework for the online and offline systems designed to detect psychological distress in patient interviews.'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.580242100Z",
     "start_time": "2023-12-01T21:03:35.557696100Z"
    }
   },
   "id": "e62694782658baee"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "texts1 = texts1.split('.')\n",
    "texts2 = texts2.split('.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.580340800Z",
     "start_time": "2023-12-01T21:03:35.568502400Z"
    }
   },
   "id": "6c89d20b47662a7a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04392171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.484381Z",
     "iopub.status.busy": "2023-12-01T13:37:43.483959Z",
     "iopub.status.idle": "2023-12-01T13:37:43.492265Z",
     "shell.execute_reply": "2023-12-01T13:37:43.490929Z"
    },
    "papermill": {
     "duration": 0.019802,
     "end_time": "2023-12-01T13:37:43.494716",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.474914",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.623110300Z",
     "start_time": "2023-12-01T21:03:35.575240400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'len of text1: 19, len of text2: 12'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'len of text1: {len(texts1)}, len of text2: {len(texts2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf74f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.512827Z",
     "iopub.status.busy": "2023-12-01T13:37:43.512435Z",
     "iopub.status.idle": "2023-12-01T13:37:43.519380Z",
     "shell.execute_reply": "2023-12-01T13:37:43.518273Z"
    },
    "papermill": {
     "duration": 0.019024,
     "end_time": "2023-12-01T13:37:43.521789",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.502765",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.624112700Z",
     "start_time": "2023-12-01T21:03:35.587856300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Word2vec is a technique for natural language processing (NLP) published in 2013',\n ' The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text',\n ' Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence',\n ' As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector',\n ' The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors',\n ' Word2vec is a group of related models that are used to produce word embeddings',\n ' These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words',\n ' Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space',\n ' Word2vec can utilize either of two model architectures to produce these distributed representations of words: continuously sliding bag-of-words (CBOW) or continuously sliding skip-gram',\n ' In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus',\n ' The CBOW can be viewed as a ‘fill in the blank’ task, where the word embedding represents the way the word influences the relative probabilities of other words in the context window',\n ' Words which are semantically similar should influence these probabilities in similar ways, because semantically similar words should be used in similar contexts',\n ' The order of context words does not influence prediction (bag-of-words assumption)',\n ' In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words',\n '[1][2] The skip-gram architecture weighs nearby context words more heavily than more distant context words',\n \" According to the authors' note,[3] CBOW is faster while skip-gram does a better job for infrequent words\",\n ' After the model has trained, the learned word embeddings are positioned in the vector space such that words that share common contexts in the corpus — that is, words that are semantically and syntactically similar — are located close to one another in the space',\n '[1] More dissimilar words are located farther from one another in the space',\n '']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['GloVe, coined from Global Vectors, is a model for distributed word representation',\n ' The model is an unsupervised learning algorithm for obtaining vector representations for words',\n ' This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity',\n '[1] Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space',\n ' It is developed as an open-source project at Stanford[2] and was launched in 2014',\n ' As log-bilinear regression model for unsupervised learning of word representations, it combines the features of two model families, namely the global matrix factorization and local context window methods',\n '[3] GloVe can be used to find relations between words like synonyms, company-product relations, zip codes and cities, etc',\n ' However, the unsupervised learning algorithm is not effective in identifying homographs, that is, words with the same spelling and different meanings',\n ' This is as the unsupervised learning algorithm calculates a single set of vectors for words with the same morphological structure',\n '[4] The algorithm is also used by the SpaCy library to build semantic word embedding features, while computing the top list words that match with distance measures such as cosine similarity and Euclidean distance approach',\n '[5] GloVe was also used as the word representation framework for the online and offline systems designed to detect psychological distress in patient interviews',\n '']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.624112700Z",
     "start_time": "2023-12-01T21:03:35.593817400Z"
    }
   },
   "id": "488abb9851ea9b5b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ebfaab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.539560Z",
     "iopub.status.busy": "2023-12-01T13:37:43.539181Z",
     "iopub.status.idle": "2023-12-01T13:37:43.550176Z",
     "shell.execute_reply": "2023-12-01T13:37:43.549092Z"
    },
    "papermill": {
     "duration": 0.022867,
     "end_time": "2023-12-01T13:37:43.552555",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.529688",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.624112700Z",
     "start_time": "2023-12-01T21:03:35.600624600Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(TEXT: str, punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_“~''', stop_words=None) -> list:\n",
    "    \"\"\"\n",
    "    A method to preprocess a text\n",
    "    \"\"\"\n",
    "    if TEXT == '':\n",
    "        return\n",
    "    if stop_words is None:\n",
    "        stop_words = ['and', 'a', 'is', 'the', 'in', 'be', 'will', 'was', 'but', 'this', 'were', 'with', 'of', 'also', 'on', '.', 'for', 'any', 'its', 'and', 'are', 'from', 'both', 'as']\n",
    "\n",
    "    for x in TEXT.lower():\n",
    "        if x in punctuations:\n",
    "            TEXT = TEXT.replace(x, \"\")\n",
    "\n",
    "    # Removing words that have numbers in them\n",
    "    TEXT = re.sub(r'\\w*\\d\\w*', '', TEXT)\n",
    "\n",
    "    # Removing digits\n",
    "    TEXT = re.sub(r'[0-9]+', '', TEXT)\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    TEXT = re.sub(r'\\s+', ' ', TEXT).strip()\n",
    "\n",
    "    # Setting every word to lower\n",
    "    TEXT = TEXT.lower()\n",
    "\n",
    "    # Converting all our text to a list \n",
    "    TEXT = TEXT.split(' ')\n",
    "\n",
    "    # Dropping empty strings\n",
    "    TEXT = [x for x in TEXT if x != '']\n",
    "\n",
    "    # Dropping stop words\n",
    "    TEXT = [x for x in TEXT if x not in stop_words]\n",
    "\n",
    "    return TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1fff23c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.570687Z",
     "iopub.status.busy": "2023-12-01T13:37:43.570297Z",
     "iopub.status.idle": "2023-12-01T13:37:43.578973Z",
     "shell.execute_reply": "2023-12-01T13:37:43.577725Z"
    },
    "papermill": {
     "duration": 0.020683,
     "end_time": "2023-12-01T13:37:43.581368",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.560685",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:35.624112700Z",
     "start_time": "2023-12-01T21:03:35.618095800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_training_data(texts: list, window=2):\n",
    "    # Defining the window for context\n",
    "    # window = 2\n",
    "\n",
    "    # Creating a placeholder for the scanning of the word list\n",
    "    word_lists = []\n",
    "    all_text = []\n",
    "    for text in texts:\n",
    "        if text is None:\n",
    "            continue\n",
    "        # Cleaning the text\n",
    "        text = text_preprocessing(text)\n",
    "        print (text)\n",
    "\n",
    "        # Appending to the all text lists\n",
    "        all_text += text\n",
    "\n",
    "        # Creating a context dictionary\n",
    "        for i, word in enumerate(text):\n",
    "            for w in range(window):\n",
    "                # Getting the context that is ahead by *window* words\n",
    "                if i + 1 + w < len(text):\n",
    "                    word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "                # Getting the context that is behind by *window* words\n",
    "                if i - w - 1 >= 0:\n",
    "                    word_lists.append([word] + [text[(i - w - 1)]])\n",
    "    return word_lists, all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30fed9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.599609Z",
     "iopub.status.busy": "2023-12-01T13:37:43.599230Z",
     "iopub.status.idle": "2023-12-01T13:37:43.612160Z",
     "shell.execute_reply": "2023-12-01T13:37:43.610867Z"
    },
    "papermill": {
     "duration": 0.025201,
     "end_time": "2023-12-01T13:37:43.614636",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.589435",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:37.213985700Z",
     "start_time": "2023-12-01T21:03:35.621112400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technique', 'natural', 'language', 'processing', 'nlp', 'published']\n",
      "['algorithm', 'uses', 'neural', 'network', 'model', 'to', 'learn', 'word', 'associations', 'large', 'corpus', 'text']\n",
      "['once', 'trained', 'such', 'model', 'can', 'detect', 'synonymous', 'words', 'or', 'suggest', 'additional', 'words', 'partial', 'sentence']\n",
      "['name', 'implies', 'represents', 'each', 'distinct', 'word', 'particular', 'list', 'numbers', 'called', 'vector']\n",
      "['vectors', 'chosen', 'carefully', 'such', 'that', 'they', 'capture', 'semantic', 'syntactic', 'qualities', 'words', 'such', 'simple', 'mathematical', 'function', 'cosine', 'similarity', 'can', 'indicate', 'level', 'semantic', 'similarity', 'between', 'words', 'represented', 'by', 'those', 'vectors']\n",
      "['group', 'related', 'models', 'that', 'used', 'to', 'produce', 'word', 'embeddings']\n",
      "['these', 'models', 'shallow', 'twolayer', 'neural', 'networks', 'that', 'trained', 'to', 'reconstruct', 'linguistic', 'contexts', 'words']\n",
      "['takes', 'input', 'large', 'corpus', 'text', 'produces', 'vector', 'space', 'typically', 'several', 'hundred', 'dimensions', 'each', 'unique', 'word', 'corpus', 'being', 'assigned', 'corresponding', 'vector', 'space']\n",
      "['can', 'utilize', 'either', 'two', 'model', 'architectures', 'to', 'produce', 'these', 'distributed', 'representations', 'words', 'continuously', 'sliding', 'bagofwords', 'cbow', 'or', 'continuously', 'sliding', 'skipgram']\n",
      "['architectures', 'considers', 'individual', 'words', 'sliding', 'context', 'window', 'it', 'iterates', 'over', 'corpus']\n",
      "['cbow', 'can', 'viewed', '‘fill', 'blank’', 'task', 'where', 'word', 'embedding', 'represents', 'way', 'word', 'influences', 'relative', 'probabilities', 'other', 'words', 'context', 'window']\n",
      "['words', 'which', 'semantically', 'similar', 'should', 'influence', 'these', 'probabilities', 'similar', 'ways', 'because', 'semantically', 'similar', 'words', 'should', 'used', 'similar', 'contexts']\n",
      "['order', 'context', 'words', 'does', 'not', 'influence', 'prediction', 'bagofwords', 'assumption']\n",
      "['continuous', 'skipgram', 'architecture', 'model', 'uses', 'current', 'word', 'to', 'predict', 'surrounding', 'window', 'context', 'words']\n",
      "['skipgram', 'architecture', 'weighs', 'nearby', 'context', 'words', 'more', 'heavily', 'than', 'more', 'distant', 'context', 'words']\n",
      "['according', 'to', 'authors', 'cbow', 'faster', 'while', 'skipgram', 'does', 'better', 'job', 'infrequent', 'words']\n",
      "['after', 'model', 'has', 'trained', 'learned', 'word', 'embeddings', 'positioned', 'vector', 'space', 'such', 'that', 'words', 'that', 'share', 'common', 'contexts', 'corpus', '—', 'that', 'words', 'that', 'semantically', 'syntactically', 'similar', '—', 'located', 'close', 'to', 'one', 'another', 'space']\n",
      "['more', 'dissimilar', 'words', 'located', 'farther', 'one', 'another', 'space']\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m word_lists, all_text \u001B[38;5;241m=\u001B[39m get_training_data(texts1)\n\u001B[0;32m      2\u001B[0m word_lists\n",
      "Cell \u001B[1;32mIn[9], line 16\u001B[0m, in \u001B[0;36mget_training_data\u001B[1;34m(texts, window)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m (text)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Appending to the all text lists\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m all_text \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m text\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Creating a context dictionary\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(text):\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "word_lists, all_text = get_training_data(texts1)\n",
    "word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:37.234292400Z",
     "start_time": "2023-12-01T21:03:37.216296Z"
    }
   },
   "id": "a67f4701aaea78f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cd93f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.634290Z",
     "iopub.status.busy": "2023-12-01T13:37:43.633849Z",
     "iopub.status.idle": "2023-12-01T13:37:43.640328Z",
     "shell.execute_reply": "2023-12-01T13:37:43.639253Z"
    },
    "papermill": {
     "duration": 0.019596,
     "end_time": "2023-12-01T13:37:43.642784",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.623188",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.219294Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_unique_word_dict(TEXT: list) -> dict:\n",
    "    \"\"\"\n",
    "    A method that creates a dictionary where the keys are unique words\n",
    "    and key values are indices\n",
    "    \"\"\"\n",
    "    # Getting all the unique words from our text and sorting them alphabetically\n",
    "    Words = list(set(TEXT))\n",
    "    Words.sort()\n",
    "\n",
    "    # Creating the dictionary for the unique words\n",
    "    UniqueWordDict = {}\n",
    "    for i, Word in enumerate(Words):\n",
    "        UniqueWordDict.update({Word: i})\n",
    "\n",
    "    return UniqueWordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e144546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.661520Z",
     "iopub.status.busy": "2023-12-01T13:37:43.661129Z",
     "iopub.status.idle": "2023-12-01T13:37:43.667177Z",
     "shell.execute_reply": "2023-12-01T13:37:43.665944Z"
    },
    "papermill": {
     "duration": 0.018563,
     "end_time": "2023-12-01T13:37:43.669969",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.651406",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.221293200Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_word_dict = create_unique_word_dict(all_text)\n",
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "unique_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065ff09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.689557Z",
     "iopub.status.busy": "2023-12-01T13:37:43.689167Z",
     "iopub.status.idle": "2023-12-01T13:37:43.694967Z",
     "shell.execute_reply": "2023-12-01T13:37:43.693834Z"
    },
    "papermill": {
     "duration": 0.018983,
     "end_time": "2023-12-01T13:37:43.697821",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.678838",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.225292200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting all the unique words\n",
    "words = list(unique_word_dict.keys())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862ca2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.717735Z",
     "iopub.status.busy": "2023-12-01T13:37:43.716779Z",
     "iopub.status.idle": "2023-12-01T13:37:43.735114Z",
     "shell.execute_reply": "2023-12-01T13:37:43.734021Z"
    },
    "papermill": {
     "duration": 0.032739,
     "end_time": "2023-12-01T13:37:43.739535",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.706796",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.228293300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the X and Y matrices using one hot encoding\n",
    "print(n_words)\n",
    "X = []\n",
    "Y = []\n",
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    print(word_list)\n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "    # print (word_list)\n",
    "    print(word_list[0], main_word_index)\n",
    "    print(word_list[1], context_word_index)\n",
    "\n",
    "    # Creating the placeholders\n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words\n",
    "    Y_row[context_word_index] = 1\n",
    "\n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43771f",
   "metadata": {
    "papermill": {
     "duration": 0.009425,
     "end_time": "2023-12-01T13:37:43.758373",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.748948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Converting the matrices into a sparse format because the vast majority of the data are 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d4d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.782143Z",
     "iopub.status.busy": "2023-12-01T13:37:43.781719Z",
     "iopub.status.idle": "2023-12-01T13:37:43.815306Z",
     "shell.execute_reply": "2023-12-01T13:37:43.814441Z"
    },
    "papermill": {
     "duration": 0.049424,
     "end_time": "2023-12-01T13:37:43.817528",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.768104",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.231293600Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10aa3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.839567Z",
     "iopub.status.busy": "2023-12-01T13:37:43.839143Z",
     "iopub.status.idle": "2023-12-01T13:37:43.874201Z",
     "shell.execute_reply": "2023-12-01T13:37:43.872870Z"
    },
    "papermill": {
     "duration": 0.049575,
     "end_time": "2023-12-01T13:37:43.877101",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.827526",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:37.234292400Z",
     "start_time": "2023-12-01T21:03:37.234292400Z"
    }
   },
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e63c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:43.900399Z",
     "iopub.status.busy": "2023-12-01T13:37:43.899980Z",
     "iopub.status.idle": "2023-12-01T13:37:44.005427Z",
     "shell.execute_reply": "2023-12-01T13:37:44.003859Z"
    },
    "papermill": {
     "duration": 0.120392,
     "end_time": "2023-12-01T13:37:44.008233",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.887841",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.236292100Z"
    }
   },
   "outputs": [],
   "source": [
    "XX = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "YY = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "print(XX.shape)\n",
    "print(YY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148425d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:44.032351Z",
     "iopub.status.busy": "2023-12-01T13:37:44.031921Z",
     "iopub.status.idle": "2023-12-01T13:37:44.039845Z",
     "shell.execute_reply": "2023-12-01T13:37:44.038498Z"
    },
    "papermill": {
     "duration": 0.02323,
     "end_time": "2023-12-01T13:37:44.042461",
     "exception": false,
     "start_time": "2023-12-01T13:37:44.019231",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.239291400Z"
    }
   },
   "outputs": [],
   "source": [
    "def CreateModel():\n",
    "    # Defining the size of the embedding\n",
    "    embed_size = 2\n",
    "    # Defining the neural network\n",
    "\n",
    "    # inp = Input(shape=(X.shape[1],))\n",
    "    inp = Input(shape=XX.shape[1])  # 21\n",
    "    x = Dense(units=embed_size, activation='linear')(inp)\n",
    "    # x = Dense(units=21, activation='softmax')(x)\n",
    "    x = Dense(units=YY.shape[1], activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cda6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:37:44.066419Z",
     "iopub.status.busy": "2023-12-01T13:37:44.065990Z",
     "iopub.status.idle": "2023-12-01T13:38:33.334050Z",
     "shell.execute_reply": "2023-12-01T13:38:33.333052Z"
    },
    "papermill": {
     "duration": 49.28302,
     "end_time": "2023-12-01T13:38:33.336416",
     "exception": false,
     "start_time": "2023-12-01T13:37:44.053396",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.242292500Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CreateModel()\n",
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    x=XX,\n",
    "    y=YY,\n",
    "    batch_size=64,\n",
    "    epochs=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['accuracy'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.244295700Z"
    }
   },
   "id": "40bef5b618bff8c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268a6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:38:34.517143Z",
     "iopub.status.busy": "2023-12-01T13:38:34.516157Z",
     "iopub.status.idle": "2023-12-01T13:38:34.533099Z",
     "shell.execute_reply": "2023-12-01T13:38:34.531113Z"
    },
    "papermill": {
     "duration": 0.620854,
     "end_time": "2023-12-01T13:38:34.535985",
     "exception": false,
     "start_time": "2023-12-01T13:38:33.915131",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.246296Z"
    }
   },
   "outputs": [],
   "source": [
    "# The input layer\n",
    "\n",
    "weights = model.get_weights()[0]  # 21*2\n",
    "print(weights.shape)\n",
    "print(weights[1][1])\n",
    "print(weights)\n",
    "\n",
    "# weights = model.get_weights()[2]\n",
    "# print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba0840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:38:35.765307Z",
     "iopub.status.busy": "2023-12-01T13:38:35.764864Z",
     "iopub.status.idle": "2023-12-01T13:38:35.770363Z",
     "shell.execute_reply": "2023-12-01T13:38:35.769201Z"
    },
    "papermill": {
     "duration": 0.651547,
     "end_time": "2023-12-01T13:38:35.772620",
     "exception": false,
     "start_time": "2023-12-01T13:38:35.121073",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.248292900Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the weight for each unique word\n",
    "embedding_dict = {}\n",
    "for word in words:  # to pick a row of weight of two values for each unique word since weights = 21*2\n",
    "    embedding_dict.update({word: weights[unique_word_dict.get(word)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e3f5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:38:36.931916Z",
     "iopub.status.busy": "2023-12-01T13:38:36.930764Z",
     "iopub.status.idle": "2023-12-01T13:38:36.941558Z",
     "shell.execute_reply": "2023-12-01T13:38:36.940565Z"
    },
    "papermill": {
     "duration": 0.592325,
     "end_time": "2023-12-01T13:38:36.943729",
     "exception": false,
     "start_time": "2023-12-01T13:38:36.351404",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.250292200Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a3e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:38:38.176627Z",
     "iopub.status.busy": "2023-12-01T13:38:38.176210Z",
     "iopub.status.idle": "2023-12-01T13:38:38.935741Z",
     "shell.execute_reply": "2023-12-01T13:38:38.934497Z"
    },
    "papermill": {
     "duration": 1.412124,
     "end_time": "2023-12-01T13:38:38.938513",
     "exception": false,
     "start_time": "2023-12-01T13:38:37.526389",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.252293700Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "i = 0\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    print(i, ' >> ', word)\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4e06d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:38:40.097973Z",
     "iopub.status.busy": "2023-12-01T13:38:40.097576Z",
     "iopub.status.idle": "2023-12-01T13:38:40.104221Z",
     "shell.execute_reply": "2023-12-01T13:38:40.103135Z"
    },
    "papermill": {
     "duration": 0.587694,
     "end_time": "2023-12-01T13:38:40.106624",
     "exception": false,
     "start_time": "2023-12-01T13:38:39.518930",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-01T21:03:37.254297800Z"
    }
   },
   "outputs": [],
   "source": [
    "# The input layer\n",
    "weights = model.get_weights()[0]\n",
    "# weights[: , 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c32a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T13:38:41.267093Z",
     "iopub.status.busy": "2023-12-01T13:38:41.266659Z",
     "iopub.status.idle": "2023-12-01T13:38:41.986896Z",
     "shell.execute_reply": "2023-12-01T13:38:41.985946Z"
    },
    "papermill": {
     "duration": 1.310399,
     "end_time": "2023-12-01T13:38:41.989224",
     "exception": false,
     "start_time": "2023-12-01T13:38:40.678825",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-01T21:03:37.319383400Z",
     "start_time": "2023-12-01T21:03:37.257294500Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "i = 0\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    print(i, ' >> ', word)\n",
    "    # coord = embedding_dict.get(word)\n",
    "    coord = embedding_dict.get(word)\n",
    "    if weights[i][0] < 0 < weights[i][1]:\n",
    "        plt.scatter(0, weights[i][1])\n",
    "        plt.annotate(word, (0, weights[i][1]))\n",
    "    else:\n",
    "        plt.scatter(weights[i][0], weights[i][1])\n",
    "        plt.annotate(word, (weights[i][0], weights[i][1]))\n",
    "    i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4091638,
     "sourceId": 7098610,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 81.736376,
   "end_time": "2023-12-01T13:38:45.297083",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-01T13:37:23.560707",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
